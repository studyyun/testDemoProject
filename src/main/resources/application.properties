server.port=9990

spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
#spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=utf-8&zeroDateTimeBehavior=convertToNull&useSSL=false&allowPublicKeyRetrieval=true
#spring.datasource.url=jdbc:oracle:thin:@//127.0.0.1:1521/test
#spring.datasource.url=jdbc:mysql://192.168.3.176:3306/neimenggu?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=utf-8&zeroDateTimeBehavior=convertToNull&useSSL=false&allowPublicKeyRetrieval=true
spring.datasource.username=root
spring.datasource.password=123456


mybatis-plus.configuration.map-underscore-to-camel-case=true
mybatis-plus.configuration.auto-mapping-behavior=full
mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl
#如果mapper.xml文件中使用了databaseId，配置文件中要配置相应值，否则该sql语句不会被加载
#mybatis-plus.configuration.database-id=mysql
mybatis-plus.mapper-locations=classpath:mapper/*.xml

mailgate.account.cache.time=180

logging.config=classpath:log4j2.xml

############## oauth2配置信息 ##############
# 唯一标识id，客户提供
oauth2.client_id=c3
# 密钥，客户提供
oauth2.client_secret=secret
# 获取code授权码地址，客户提供
oauth2.code_uri=http://192.169.1.176:8002/oauth/authorize
# 获取token令牌地址，客户提供
oauth2.token_uri=http://192.169.1.176:8002/oauth/token
# SSO注销地址，客户提供
oauth2.logout_uri=http://192.169.1.176:8002/oauth/check_token

#server.ssl.key-store=classpath:client.p12
#server.ssl.key-store-password=12345678
#server.ssl.key-store-type=PKCS12
#server.ssl.key-alias=client
#

demo.name=ooo
demo.age=11


###########【Kafka集群】###########
#spring.kafka.bootstrap-servers=192.168.3.176:9092
#spring.kafka.bootstrap-servers=120.24.162.16:9092,120.24.162.16:9093,120.24.162.16:9094
spring.kafka.bootstrap-servers=120.24.162.16:9092
############【初始化生产者配置】###########
## 重试次数
spring.kafka.producer.retries=0
## 应答级别:多少个分区副本(包括leader副本和follow副本)备份完成时向生产者发送ack确认(可选0、1、all/-1)
#acks = all/-1 : 表示kafka ISR列表中所有的副本同步数据成功，才返回消息给生产者
#acks = 0 ：表示生产者只管发送数据，不管broker接收数据的任何情况
#acks = 1 ：表示生产者发送数据后，需要在broker leader 副本写入数据成功后，返回响应
spring.kafka.producer.acks=1
## 批量大小 单位：KB
spring.kafka.producer.batch-size=2000
## 提交延时 单位：ms
spring.kafka.producer.properties.linger.ms=6000
## 当生产端积累的消息达到batch-size或接收到消息linger.ms后,生产者就会将消息提交给kafka
## linger.ms为0表示每接收到一条消息就提交给kafka,这时候batch-size其实就没用了​
## 生产端缓冲区大小
spring.kafka.producer.buffer-memory=33554432
## Kafka提供的序列化和反序列化类
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
## 自定义分区器
#spring.kafka.producer.properties.partitioner.class=com.example.demo.kafka.producer.CustomizePartitioner
#spring.kafka.producer.compression-type=gzip
############【初始化消费者配置】###########
## 默认的消费组ID
spring.kafka.consumer.properties.group.id=defaultConsumerGroup
## 是否自动提交offset，这个自动提交并不是每消费一条消息就自动提交消费位移，而是定期提交，这个定期提交的时间由参数auto.commit.interval.ms配置，默认5秒
#spring.kafka.consumer.enable-auto-commit=false
## 提交offset延时(接收到消息后多久提交offset)，和enable-auto-commit搭配使用
##spring.kafka.consumer.auto.commit.interval.ms=1000
## 当kafka中没有初始offset或offset超出范围时将自动重置offset
## earliest： 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
## latest： 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
## none： topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=latest
## 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
#spring.kafka.consumer.properties.session.timeout.ms=120000
## 消费请求超时时间
#spring.kafka.consumer.properties.request.timeout.ms=180000
## Kafka提供的序列化和反序列化类
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
## 消费端监听的topic不存在时，项目启动会报错(关掉)
spring.kafka.listener.missing-topics-fatal=false
## 设置批量消费
#spring.kafka.listener.type=batch
## 批量消费每次最多消费多少条消息
#spring.kafka.consumer.max-poll-records=100
#
#spring.kafka.listener.poll-timeout=10000
#spring.kafka.consumer.auto-commit-interval=10000

#指定redis信息 (如 host, ip, password)
spring.redis.host=120.24.162.16
spring.redis.port=6379
spring.redis.password=aliyun
